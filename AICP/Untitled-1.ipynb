{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-25 09:12:35,114 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, <START>, <STOP>\n",
      "Sentence[6]: \"George Washington went to Washington.\" â†’ [\"George Washington\"/PER, \"Washington\"/LOC]\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.nn import Classifier\n",
    "\n",
    "# make a sentence\n",
    "sentence = Sentence('George Washington went to Washington.')\n",
    "\n",
    "# load the NER tagger\n",
    "tagger = Classifier.load('ner-large')\n",
    "\n",
    "# run NER over sentence\n",
    "tagger.predict(sentence)\n",
    "\n",
    "# print the sentence with all annotations\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "import re\n",
    "doc = Document('C:\\\\Users\\\\hechu\\\\Downloads\\\\1st_Meeting_3rd_Session_Open-ended_Working_Group_on_Reducing_Space_Threats.docx')\n",
    "speeches = {}\n",
    "\n",
    "def clean(text):\n",
    "    # Remove timestamps\n",
    "    clean_text = re.sub(r'\\d{2}:\\d{2}:\\d{2}\\s*', '', text)\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "paragraphs = [clean(para.text) for para in doc.paragraphs if para.text.strip() != '']\n",
    "\n",
    "\n",
    "# Define the path for the output text file\n",
    "output_file_path = 'C:\\\\Users\\\\hechu\\\\Desktop\\\\AICP\\\\output.txt'\n",
    "\n",
    "# Write paragraphs to the text file\n",
    "with open(output_file_path, 'w', encoding='utf-8') as txt_file:\n",
    "    txt_file.write('\\n'.join(paragraphs))\n",
    "\n",
    "txt_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text file\n",
    "file_path = 'C:\\\\Users\\\\hechu\\\\Desktop\\\\AICP\\\\output.txt'  # Update with your file path\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as txt_file:\n",
    "    lines = txt_file.readlines()\n",
    "\n",
    "# Dictionary to store speeches\n",
    "speeches = {}\n",
    "current_speaker = None\n",
    "previous_speaker = None\n",
    "\n",
    "# Process each line\n",
    "for line in lines:\n",
    "    if re.match(r'Speaker \\d+', line.strip()):\n",
    "        previous_speaker = current_speaker\n",
    "        current_speaker = line.strip()\n",
    "        if current_speaker not in speeches:\n",
    "            speeches[current_speaker] = ''\n",
    "    else:\n",
    "        speeches[current_speaker] += line.strip() + ' '\n",
    "\n",
    "txt_file.close()\n",
    "# Write the consolidated speeches back to a file\n",
    "output_file_path = 'C:\\\\Users\\\\hechu\\\\Desktop\\\\AICP\\\\consolidated_output.txt'  # Update with your desired output path\n",
    "with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "    for speaker, speech in speeches.items():\n",
    "        file.write(f\"{speaker}\\n{speech}\\n\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-25 12:47:02,011 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, <START>, <STOP>\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import spacy\n",
    "\n",
    "with open('C:\\\\Users\\\\hechu\\\\Desktop\\\\AICP\\\\consolidated_output.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "sentences = [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "\n",
    "key = \"4011e47207ed418b9196929c20a002bd\"\n",
    "endpoint = \"https://eastus.api.cognitive.microsoft.com/\"\n",
    "\n",
    "# Authenticate the client using your key and endpoint \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "# Example function for recognizing entities from text\n",
    "def entity_recognition(client):\n",
    "    try:\n",
    "        documents = [{\"id\": str(idx), \"text\": sent} for idx, sent in enumerate(sentences)]\n",
    "        result = client.recognize_entities(documents = documents)\n",
    "\n",
    "        if doc in result:\n",
    "            if not doc.is_error:\n",
    "                print(\"Named Entities:\\n\")\n",
    "                for entity in doc.entities:\n",
    "                    print(\"\\tText: \\t\", entity.text, \"\\tCategory: \\t\", entity.category, \"\\tSubCategory: \\t\", entity.subcategory,\n",
    "                            \"\\n\\tConfidence Score: \\t\", round(entity.confidence_score, 2), \"\\tLength: \\t\", entity.length, \"\\tOffset: \\t\", entity.offset, \"\\n\")\n",
    "            else:\n",
    "                print(f\"Error: {doc.error}\")\n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))\n",
    "entity_recognition(client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\hechu\\\\Desktop\\\\AICP\\\\annotated_output.txt', 'w', encoding='utf-8') as txt_file:\n",
    "    txt_file.write(sentence.to_tagged_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
